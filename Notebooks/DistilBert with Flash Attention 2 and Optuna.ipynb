{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdvQgAEs8Is9I3D5v30bRU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ybg2oBaKX0Ma"},"outputs":[],"source":["!pip install transformers -U\n","!pip install accelerate -U\n","!pip install optuna -U\n","!pip install -U flash-attn --no-build-isolation #install flash attention 2\n","\n","import pandas as pd\n","import torch\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModel\n","from torch.utils.data import Dataset\n","import matplotlib.pyplot as plt\n","import nltk\n","import re\n","import optuna\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import time\n","\n","# Load the dataset\n","from google.colab import drive\n","drive.mount('/content/drive')\n","java_file_path = '/content/drive/MyDrive/Wajid Ali/FYP/Dataset/java.csv'\n","python_file_path = '/content/drive/MyDrive/Wajid Ali/FYP/Dataset/python.csv'\n","pharo_file_path = '/content/drive/MyDrive/Wajid Ali/FYP/Dataset/pharo.csv'\n","java_df = pd.read_csv(java_file_path)\n","python_df = pd.read_csv(python_file_path)\n","pharo_df = pd.read_csv(pharo_file_path)\n","\n","# Data preparation\n","java_df['category'] = java_df['category'].replace('Expand', 'java_Expand')\n","df = pd.concat([java_df, python_df, pharo_df], ignore_index=True)\n","\n","# Preprocessing of the dataset\n","def preprocess_text(text):\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    return text\n","\n","df['combo'] = df['class'] + \" \" + df['comment_sentence']\n","df['combo'] = df['combo'].apply(preprocess_text)\n","\n","# Split the dataset into training and testing data\n","train_data = df[df['partition'] == 1]\n","test_data = df[df['partition'] == 0]\n","\n","# Tokenization and Dataset preparation\n","device = \"cuda\"\n","tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n","\n","class CommentDataset(Dataset):\n","    def __init__(self, combos, labels, tokenizer, max_length=512):\n","        self.tokenizer = tokenizer\n","        self.combos = combos\n","        self.labels = labels\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.combos)\n","\n","    def __getitem__(self, idx):\n","        combo = str(self.combos[idx])\n","        label = self.labels[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            combo,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","train_dataset = CommentDataset(train_data['combo'].to_numpy(), train_data['category'].astype('category').cat.codes.to_numpy(), tokenizer)\n","test_dataset = CommentDataset(test_data['combo'].to_numpy(), test_data['category'].astype('category').cat.codes.to_numpy(), tokenizer)\n","\n","def objective(trial):\n","    # Hyperparameter optimization with Optuna\n","    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 5e-5)\n","    num_train_epochs = trial.suggest_int('num_train_epochs', 2, 4)\n","    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16, 32])\n","\n","    args = TrainingArguments(\n","        output_dir='./results',\n","        learning_rate=learning_rate,\n","        num_train_epochs=num_train_epochs,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        per_device_eval_batch_size=16,\n","        warmup_steps=500,\n","        weight_decay=0.01,\n","        evaluation_strategy='steps',\n","        logging_dir='./logs',\n","        logging_steps=10\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=args,\n","        train_dataset=train_dataset,\n","        eval_dataset=test_dataset,\n","        compute_metrics=compute_metrics\n","    )\n","    trainer.train()\n","    eval_result = trainer.evaluate()\n","    return eval_result['eval_loss']\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=3)\n","\n","best_trial = study.best_trial\n","for key, value in best_trial.params.items():\n","    print(f\"{key}: {value}\")\n","\n","# Use best hyperparameters to configure training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=best_trial.params['num_train_epochs'],\n","    per_device_train_batch_size=best_trial.params['per_device_train_batch_size'],\n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    evaluation_strategy='steps',\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    learning_rate=best_trial.params['learning_rate']\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"]}]}